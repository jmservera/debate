{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data import from the \"Congreso de los Diputados\" website\n",
        "\n",
        "This process is focused on extracting the political debates that happened in the congress and tag every intervention with the name of the politician. The main objective is to create a corpora for political profiles that can be used to train an ML Transformer."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installers\n",
        "%pip install alive_progress"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: alive_progress in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.4.1)\nRequirement already satisfied: grapheme==0.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (0.6.0)\nRequirement already satisfied: about-time==3.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (3.1.1)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1668433803169
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from html.parser import HTMLParser\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# constants\n",
        "#page = 'https://www.congreso.es/busqueda-de-publicaciones?p_p_id=publicaciones&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&_publicaciones_mode=mostrarTextoIntegro&_publicaciones_legislatura=XII&_publicaciones_id_texto=(DSCD-12-PL-4.CODI.)#(P%C3%A1gina12)'\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1668433803628
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "\n",
        "\"\"\"\n",
        "A simple parser to extract all the text of a publication from the body.\n",
        "It removes any internal script and removes special characters with the str.strip function.\n",
        "It also gets rid of pagination (Página nnn)\n",
        "\"\"\"\n",
        "class PublicationParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        #Initializing lists\n",
        "        self.lsStartTags = list()\n",
        "        self.lsEndTags = list()\n",
        "        self.lsStartEndTags = list()\n",
        "        self.lsComments = list()\n",
        "        self.lsData=list()\n",
        "        # Indicates when we are inside the body tag\n",
        "        self.inBody=False\n",
        "        # Marker for scripts\n",
        "        self.inScript=False\n",
        "\n",
        "    #HTML Parser Methods\n",
        "    def handle_starttag(self, startTag, attrs):\n",
        "        self.lsStartTags.append(startTag)\n",
        "        if(startTag==\"body\"):\n",
        "            self.inBody=True\n",
        "        if(startTag==\"script\"):\n",
        "            self.inScript=True\n",
        "\n",
        "    def handle_endtag(self, endTag):\n",
        "        self.lsEndTags.append(endTag)\n",
        "        if(endTag==\"body\"):\n",
        "            self.inBody=False\n",
        "        elif(endTag==\"script\"):\n",
        "            self.inScript=False\n",
        "\n",
        "    def handle_startendtag(self,startendTag, attrs):\n",
        "       self.lsStartEndTags.append(startendTag)\n",
        "\n",
        "    def handle_comment(self,data):\n",
        "       self.lsComments.append(data)\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        if(self.inBody and not self.inScript and data!=''):\n",
        "            if(not (data.startswith('Página ') or data.startswith('(Página') )):\n",
        "                self.lsData.append(data.strip())\n",
        "\n",
        "           \n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1668435591291
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main body extractor\n",
        "\n",
        "Gets the body and finds the start of the debate by looking at the first appearance of the word PRESIDENT*, because when someone speaks its name appears in capital letters and in the case of the chamber president the words PRESIDENTE or PRESIDENTA are used. This appearance usually indicates the start of the interventions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findStart(ls:list)->int:\n",
        "    index=0\n",
        "    for line in ls:\n",
        "        if \"PRESIDENT\" in line:\n",
        "            return index\n",
        "        index+=1\n",
        "    return -1\n",
        "\n",
        "def getPublicationText(url:str,filename:str)->str:\n",
        "    import urllib3\n",
        "    # variables\n",
        "    http = urllib3.PoolManager()\n",
        "    # Get the publication\n",
        "    response = http.request('GET', url)\n",
        "    # Parse the publication\n",
        "    parser = PublicationParser()\n",
        "    parser.feed(response.data.decode('utf-8'))\n",
        "\n",
        "    index=findStart(parser.lsData)\n",
        "    text=''\n",
        "    if(index>=0):\n",
        "        # lsData is a list of strings, so we join them all with a space\n",
        "        text=' '.join(parser.lsData[index:])        \n",
        "    else:\n",
        "        os.makedirs('data/errors', exist_ok=True)\n",
        "        with open(f'data/errors/{filename}', mode='w') as f:\n",
        "            f.write(response.data.decode('utf-8'))\n",
        "        print(\"Error: PRESIDENT* not found\")\n",
        "    return text\n",
        "\n",
        "def get_speeches(text:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the speeches from the text of a publication.\n",
        "    \"\"\"\n",
        "\n",
        "    # This regex finds the name of the politician that is speaking.\n",
        "    # The name is usually in the form of:\n",
        "    # [text]... ALL CAPS SURNAME (the title if president or candidate):\n",
        "    # So we use this simple regex to find the next ALL CAPS that may be \n",
        "    # followed by a parenthesis and ends with a colon.\n",
        "    regexfinder = r'(?:(?:[A-ZÀ-Ü,])(?:-|\\s)?)+(?:\\s*\\([^\\)]*\\))?:'\n",
        "\n",
        "    indexes=[(m.start(0),m.end(0)) for m in re.finditer(regexfinder,text, re.U|re.M)]\n",
        "\n",
        "    sentences=pd.DataFrame(columns=['Name','Text'])\n",
        "    last=len(indexes)-1\n",
        "    for i in range(len(indexes)):\n",
        "        name=text[indexes[i][0]:indexes[i][1]-1]   \n",
        "\n",
        "        firstIdx=indexes[i][1]+1\n",
        "        if(i<last):\n",
        "            lastIdx=indexes[i+1][0]\n",
        "            while(text[lastIdx]!='.' and text[lastIdx]!=')'):\n",
        "                lastIdx=lastIdx-1\n",
        "                if(lastIdx==-1):\n",
        "                    break\n",
        "        else:\n",
        "            lastIdx=len(text)\n",
        "        sentence=text[firstIdx:lastIdx]\n",
        "        sentences.loc[len(sentences)]=[name,sentence]\n",
        "    return sentences\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1668435591439
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_speeches(term:pd.DataFrame)->pd.DataFrame:\n",
        "    from alive_progress import alive_bar\n",
        "    import os\n",
        "\n",
        "    with alive_bar(len(term),title=f'importing Term {term.loc[0][\"term\"]}',force_tty=True) as bar:\n",
        "        speeches_ds = None\n",
        "        bar.text='open'\n",
        "        for index,r in term.iterrows():\n",
        "            if os.path.isfile(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt'):\n",
        "                bar.text=f'c-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "                with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt','r') as f:\n",
        "                    text=f.read()\n",
        "            else:\n",
        "                bar.text=f'u-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "                text=getPublicationText(r[\"url\"],f'{r[\"term\"]}/{r[\"fecha\"]}.txt')\n",
        "                os.makedirs(f'data/pagecache/{r[\"term\"]}',exist_ok=True)\n",
        "                with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt',mode='w') as f:\n",
        "                    f.write(text)\n",
        "\n",
        "            bar.text=f'Parse {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            speeches=get_speeches(text)\n",
        "            bar.text='Append data {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            speeches[\"Date\"]=r[\"fecha\"]\n",
        "            speeches[\"Term\"]=r[\"term\"]\n",
        "            bar.text=f'Merge {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            if speeches_ds is None:\n",
        "                speeches_ds=speeches\n",
        "            else:\n",
        "                speeches_ds=pd.concat([speeches_ds,speeches])\n",
        "            bar()\n"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1668435591622
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#speeches_ds.to_csv('data/speeches_termV.csv',index=False)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1668435591899
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "os.makedirs('data/debates', exist_ok=True)\n",
        "\n",
        "for term_file in glob.glob('data/terms/*.csv'):\n",
        "    base_name = os.path.basename(term_file)\n",
        "    current_term=pd.read_csv(term_file)\n",
        "    term_id=current_term.loc[0]['term']\n",
        "    ds=parse_speeches(current_term)\n",
        "    ds.to_csv(f'data/debates/speeches_term_{term_id}.csv',index=False)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "importing Term X |███▌                                    | ▁▃▅ 28/315 [9%] in 9s (3.1/s, eta: 1:31) Parse X-20120529   \rimporting Term X |████▋                                   | ▂▂▄ 36/315 [11%] in 12s (2.9/s, eta: 1:36) Merge X-20120412"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1668435581988
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "77df4a96c0be8bd891423502f384941e461f81b0e784fdbc8bb136153927d3d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}