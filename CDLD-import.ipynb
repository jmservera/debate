{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data import from the \"Congreso de los Diputados\" website\n",
        "\n",
        "This process is focused on extracting the political debates that happened in the congress and tag every intervention with the name of the politician. The main objective is to create a corpora for political profiles that can be used to train an ML Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3485.85s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alive_progress in ./.venv/lib/python3.10/site-packages (2.4.1)\n",
            "Requirement already satisfied: grapheme==0.6.0 in ./.venv/lib/python3.10/site-packages (from alive_progress) (0.6.0)\n",
            "Requirement already satisfied: about-time==3.1.1 in ./.venv/lib/python3.10/site-packages (from alive_progress) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# installers\n",
        "%pip install alive_progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1668039224762
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from html.parser import HTMLParser\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# constants\n",
        "#page = 'https://www.congreso.es/busqueda-de-publicaciones?p_p_id=publicaciones&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&_publicaciones_mode=mostrarTextoIntegro&_publicaciones_legislatura=XII&_publicaciones_id_texto=(DSCD-12-PL-4.CODI.)#(P%C3%A1gina12)'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1668039224892
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# functions\n",
        "\n",
        "\"\"\"\n",
        "A simple parser to extract all the text of a publication from the body.\n",
        "It removes any internal script and removes special characters with the str.strip function.\n",
        "It also gets rid of pagination (Página nnn)\n",
        "\"\"\"\n",
        "class PublicationParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        #Initializing lists\n",
        "        self.lsStartTags = list()\n",
        "        self.lsEndTags = list()\n",
        "        self.lsStartEndTags = list()\n",
        "        self.lsComments = list()\n",
        "        self.lsData=list()\n",
        "        # Indicates when we are inside the body tag\n",
        "        self.inBody=False\n",
        "        # Marker for scripts\n",
        "        self.inScript=False\n",
        "\n",
        "    #HTML Parser Methods\n",
        "    def handle_starttag(self, startTag, attrs):\n",
        "        self.lsStartTags.append(startTag)\n",
        "        if(startTag==\"body\"):\n",
        "            self.inBody=True\n",
        "        if(startTag==\"script\"):\n",
        "            self.inScript=True\n",
        "\n",
        "    def handle_endtag(self, endTag):\n",
        "        self.lsEndTags.append(endTag)\n",
        "        if(endTag==\"body\"):\n",
        "            self.inBody=False\n",
        "        elif(endTag==\"script\"):\n",
        "            self.inScript=False\n",
        "\n",
        "    def handle_startendtag(self,startendTag, attrs):\n",
        "       self.lsStartEndTags.append(startendTag)\n",
        "\n",
        "    def handle_comment(self,data):\n",
        "       self.lsComments.append(data)\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        if(self.inBody and not self.inScript and data!=''):\n",
        "            if(not (data.startswith('Página ') or data.startswith('(Página') )):\n",
        "                self.lsData.append(data.strip())\n",
        "\n",
        "           \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Main body extractor\n",
        "\n",
        "Gets the body and finds the start of the debate by looking at the first appearance of the word PRESIDENT*, because when someone speaks its name appears in capital letters and in the case of the chamber president the words PRESIDENTE or PRESIDENTA are used. This appearance usually indicates the start of the interventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1668039227690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def findStart(ls:list)->int:\n",
        "    index=0\n",
        "    for line in ls:\n",
        "        if \"PRESIDENT\" in line:\n",
        "            return index\n",
        "        index+=1\n",
        "    return -1\n",
        "\n",
        "def getPublicationText(url):\n",
        "    import urllib3\n",
        "    # variables\n",
        "    http = urllib3.PoolManager()\n",
        "    # Get the publication\n",
        "    response = http.request('GET', url)\n",
        "    # Parse the publication\n",
        "    parser = PublicationParser()\n",
        "    parser.feed(response.data.decode('utf-8'))\n",
        "\n",
        "    index=findStart(parser.lsData)\n",
        "    if(index>=0):\n",
        "        # lsData is a list of strings, so we join them all with a space\n",
        "        text=' '.join(parser.lsData[index:])        \n",
        "    else:\n",
        "        print(\"Error: PRESIDENT* not found\")\n",
        "    return text\n",
        "\n",
        "def get_speeches(text:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the speeches from the text of a publication.\n",
        "    \"\"\"\n",
        "\n",
        "    # This regex finds the name of the politician that is speaking.\n",
        "    # The name is usually in the form of:\n",
        "    # [text]... ALL CAPS SURNAME (the title if president or candidate):\n",
        "    # So we use this simple regex to find the next ALL CAPS that may be \n",
        "    # followed by a parenthesis and ends with a colon.\n",
        "    regexfinder = r'(?:(?:[A-ZÀ-Ü,])(?:-|\\s)?)+(?:\\s*\\((?:[A-ZÀ-Ü-a-z-à-ü]*\\s?)*\\))?:'\n",
        "\n",
        "    indexes=[(m.start(0),m.end(0)) for m in re.finditer(regexfinder,text, re.U|re.M)]\n",
        "\n",
        "    sentences=pd.DataFrame(columns=['Name','Text'])\n",
        "    last=len(indexes)-1\n",
        "    for i in range(len(indexes)):\n",
        "        name=text[indexes[i][0]:indexes[i][1]-1]   \n",
        "\n",
        "        firstIdx=indexes[i][1]+1\n",
        "        if(i<last):\n",
        "            lastIdx=indexes[i+1][0]\n",
        "            while(text[lastIdx]!='.' and text[lastIdx]!=')'):\n",
        "                lastIdx=lastIdx-1\n",
        "                if(lastIdx==-1):\n",
        "                    break\n",
        "        else:\n",
        "            lastIdx=len(text)\n",
        "        sentence=text[firstIdx:lastIdx]\n",
        "        sentences.loc[len(sentences)]=[name,sentence]\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1668039227954
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing Term 5 |⚠︎                                       | (!) 0/197 [0%] in 18.3s (0.00/s)                            \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [31], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         f\u001b[39m.\u001b[39mwrite(text)\n\u001b[1;32m     20\u001b[0m bar\u001b[39m.\u001b[39mtext\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mParse \u001b[39m\u001b[39m{\u001b[39;00mr[\u001b[39m\"\u001b[39m\u001b[39mterm\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mr[\u001b[39m\"\u001b[39m\u001b[39mfecha\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m speeches\u001b[39m=\u001b[39mget_speeches(text)\n\u001b[1;32m     22\u001b[0m speeches[\u001b[39m\"\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mr[\u001b[39m\"\u001b[39m\u001b[39mfecha\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m speeches[\u001b[39m\"\u001b[39m\u001b[39mTerm\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mr[\u001b[39m\"\u001b[39m\u001b[39mterm\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "Cell \u001b[0;32mIn [29], line 39\u001b[0m, in \u001b[0;36mget_speeches\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# This regex finds the name of the politician that is speaking.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# The name is usually in the form of:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# [text]... ALL CAPS SURNAME (the title if president or candidate):\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# So we use this simple regex to find the next ALL CAPS that may be followed\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# by a parenthesis and ends with a colon.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m regexfinder\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?:(?:[A-ZÀ-Ü])(?:-|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms)?)+(?:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m((?:[A-ZÀ-Ü-a-z-à-ü]*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms?)*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m))?:\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 39\u001b[0m indexes\u001b[39m=\u001b[39m[(m\u001b[39m.\u001b[39mstart(\u001b[39m0\u001b[39m),m\u001b[39m.\u001b[39mend(\u001b[39m0\u001b[39m)) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m re\u001b[39m.\u001b[39mfinditer(regexfinder,text, re\u001b[39m.\u001b[39mU\u001b[39m|\u001b[39mre\u001b[39m.\u001b[39mM)]\n\u001b[1;32m     41\u001b[0m sentences\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     42\u001b[0m last\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(indexes)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
            "Cell \u001b[0;32mIn [29], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# This regex finds the name of the politician that is speaking.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# The name is usually in the form of:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# [text]... ALL CAPS SURNAME (the title if president or candidate):\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# So we use this simple regex to find the next ALL CAPS that may be followed\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# by a parenthesis and ends with a colon.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m regexfinder\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?:(?:[A-ZÀ-Ü])(?:-|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms)?)+(?:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m((?:[A-ZÀ-Ü-a-z-à-ü]*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms?)*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m))?:\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 39\u001b[0m indexes\u001b[39m=\u001b[39m[(m\u001b[39m.\u001b[39mstart(\u001b[39m0\u001b[39m),m\u001b[39m.\u001b[39mend(\u001b[39m0\u001b[39m)) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m re\u001b[39m.\u001b[39mfinditer(regexfinder,text, re\u001b[39m.\u001b[39mU\u001b[39m|\u001b[39mre\u001b[39m.\u001b[39mM)]\n\u001b[1;32m     41\u001b[0m sentences\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     42\u001b[0m last\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(indexes)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from alive_progress import alive_bar\n",
        "import os\n",
        "\n",
        "term5 = pd.read_csv('data/terms/term_5.csv')\n",
        "\n",
        "with alive_bar(len(term5),title=f'importing Term 5',force_tty=True) as bar:\n",
        "    speeches_ds = None\n",
        "    for index,r in term5.iterrows():\n",
        "        if os.path.isfile(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt'):\n",
        "            bar.text=f'c-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt','r') as f:\n",
        "                text=f.read()\n",
        "        else:\n",
        "            bar.text=f'u-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            text=getPublicationText(r[\"url\"])\n",
        "            os.makedirs(f'data/pagecache/{r[\"term\"]}',exist_ok=True)\n",
        "            with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt',mode='w') as f:\n",
        "                f.write(text)\n",
        "\n",
        "        bar.text=f'Parse {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "        speeches=get_speeches(text)\n",
        "        speeches[\"Date\"]=r[\"fecha\"]\n",
        "        speeches[\"Term\"]=r[\"term\"]\n",
        "        if speeches_ds is None:\n",
        "            speeches_ds=speeches\n",
        "        else:\n",
        "            speeches_ds=pd.concat([speeches_ds,speeches])\n",
        "        bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "speeches_ds.to_csv('data/speeches_termV.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# term5 = pd.read_csv('data/terms/term_5.csv')\n",
        "\n",
        "# for index,r in term5.iterrows():    \n",
        "#     text=getPublicationText(r[\"url\"])\n",
        "#     speeches=get_speeches(text)\n",
        "#     if(index>1):\n",
        "#         break\n",
        "\n",
        "# speeches.head()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "77df4a96c0be8bd891423502f384941e461f81b0e784fdbc8bb136153927d3d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
