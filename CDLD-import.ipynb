{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data import from the \"Congreso de los Diputados\" website\n",
        "\n",
        "This process is focused on extracting the political debates that happened in the congress and tag every intervention with the name of the politician. The main objective is to create a corpora for political profiles that can be used to train an ML Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1719.03s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alive_progress in ./.venv/lib/python3.10/site-packages (2.4.1)\n",
            "Requirement already satisfied: grapheme==0.6.0 in ./.venv/lib/python3.10/site-packages (from alive_progress) (0.6.0)\n",
            "Requirement already satisfied: about-time==3.1.1 in ./.venv/lib/python3.10/site-packages (from alive_progress) (3.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# installers\n",
        "%pip install alive_progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1668039224762
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from html.parser import HTMLParser\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# constants\n",
        "#page = 'https://www.congreso.es/busqueda-de-publicaciones?p_p_id=publicaciones&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&_publicaciones_mode=mostrarTextoIntegro&_publicaciones_legislatura=XII&_publicaciones_id_texto=(DSCD-12-PL-4.CODI.)#(P%C3%A1gina12)'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1668039224892
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# functions\n",
        "\n",
        "\"\"\"\n",
        "A simple parser to extract all the text of a publication from the body.\n",
        "It removes any internal script and removes special characters with the str.strip function.\n",
        "It also gets rid of pagination (Página nnn)\n",
        "\"\"\"\n",
        "class PublicationParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        #Initializing lists\n",
        "        self.lsStartTags = list()\n",
        "        self.lsEndTags = list()\n",
        "        self.lsStartEndTags = list()\n",
        "        self.lsComments = list()\n",
        "        self.lsData=list()\n",
        "        # Indicates when we are inside the body tag\n",
        "        self.inBody=False\n",
        "        # Marker for scripts\n",
        "        self.inScript=False\n",
        "\n",
        "    #HTML Parser Methods\n",
        "    def handle_starttag(self, startTag, attrs):\n",
        "        self.lsStartTags.append(startTag)\n",
        "        if(startTag==\"body\"):\n",
        "            self.inBody=True\n",
        "        if(startTag==\"script\"):\n",
        "            self.inScript=True\n",
        "\n",
        "    def handle_endtag(self, endTag):\n",
        "        self.lsEndTags.append(endTag)\n",
        "        if(endTag==\"body\"):\n",
        "            self.inBody=False\n",
        "        elif(endTag==\"script\"):\n",
        "            self.inScript=False\n",
        "\n",
        "    def handle_startendtag(self,startendTag, attrs):\n",
        "       self.lsStartEndTags.append(startendTag)\n",
        "\n",
        "    def handle_comment(self,data):\n",
        "       self.lsComments.append(data)\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        if(self.inBody and not self.inScript and data!=''):\n",
        "            if(not (data.startswith('Página ') or data.startswith('(Página') )):\n",
        "                self.lsData.append(data.strip())\n",
        "\n",
        "           \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Main body extractor\n",
        "\n",
        "Gets the body and finds the start of the debate by looking at the first appearance of the word PRESIDENT*, because when someone speaks its name appears in capital letters and in the case of the chamber president the words PRESIDENTE or PRESIDENTA are used. This appearance usually indicates the start of the interventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1668039227690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def findStart(ls:list)->int:\n",
        "    index=0\n",
        "    for line in ls:\n",
        "        if \"PRESIDENT\" in line:\n",
        "            return index\n",
        "        index+=1\n",
        "    return -1\n",
        "\n",
        "def getPublicationText(url):\n",
        "    import urllib3\n",
        "    # variables\n",
        "    http = urllib3.PoolManager()\n",
        "    # Get the publication\n",
        "    response = http.request('GET', url)\n",
        "    # Parse the publication\n",
        "    parser = PublicationParser()\n",
        "    parser.feed(response.data.decode('utf-8'))\n",
        "\n",
        "    index=findStart(parser.lsData)\n",
        "    if(index>=0):\n",
        "        # lsData is a list of strings, so we join them all with a space\n",
        "        text=' '.join(parser.lsData[index:])        \n",
        "    else:\n",
        "        print(\"Error: PRESIDENT* not found\")\n",
        "    return text\n",
        "\n",
        "def get_speeches(text:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the speeches from the text of a publication.\n",
        "    \"\"\"\n",
        "\n",
        "    # This regex finds the name of the politician that is speaking.\n",
        "    # The name is usually in the form of:\n",
        "    # [text]... ALL CAPS SURNAME (the title if president or candidate):\n",
        "    # So we use this simple regex to find the next ALL CAPS that may be followed\n",
        "    # by a parenthesis and ends with a colon.\n",
        "    regexfinder=r'(?:(?:[A-ZÀ-Ü])(?:-|\\s)?)+(?:\\s*\\((?:[A-ZÀ-Ü-a-z-à-ü]*\\s?)*\\))?:'\n",
        "\n",
        "    indexes=[(m.start(0),m.end(0)) for m in re.finditer(regexfinder,text, re.U|re.M)]\n",
        "\n",
        "    sentences=pd.DataFrame(columns=['Name','Text'])\n",
        "    last=len(indexes)-1\n",
        "    for i in range(len(indexes)):\n",
        "        name=text[indexes[i][0]:indexes[i][1]-1]   \n",
        "\n",
        "        firstIdx=indexes[i][1]+1\n",
        "        if(i<last):\n",
        "            lastIdx=indexes[i+1][0]\n",
        "            while(text[lastIdx]!='.'):\n",
        "                lastIdx=lastIdx-1\n",
        "                if(lastIdx==-1):\n",
        "                    break\n",
        "        else:\n",
        "            lastIdx=len(text)\n",
        "        sentence=text[firstIdx:lastIdx]\n",
        "        sentences.loc[len(sentences)]=[name,sentence]\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1668039227954
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing Term 5 |████████████████████████████████████████| 197/197 [100%] in 4:37.7 (0.71/s)                           \n"
          ]
        }
      ],
      "source": [
        "from alive_progress import alive_bar\n",
        "import os\n",
        "\n",
        "term5 = pd.read_csv('data/terms/term_5.csv')\n",
        "\n",
        "with alive_bar(len(term5),title=f'importing Term 5',force_tty=True) as bar:\n",
        "    speeches_ds = None\n",
        "    for index,r in term5.iterrows():\n",
        "        if os.path.isfile(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt'):\n",
        "            bar.text=f'c-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt','r') as f:\n",
        "                text=f.read()\n",
        "        else:\n",
        "            bar.text=f'u-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            text=getPublicationText(r[\"url\"])\n",
        "            os.makedirs(f'data/pagecache/{r[\"term\"]}',exist_ok=True)\n",
        "            with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt',mode='w') as f:\n",
        "                f.write(text)\n",
        "\n",
        "        speeches=get_speeches(text)\n",
        "        speeches[\"Date\"]=r[\"fecha\"]\n",
        "        speeches[\"Term\"]=r[\"term\"]\n",
        "        if speeches_ds is None:\n",
        "            speeches_ds=speeches\n",
        "        else:\n",
        "            speeches_ds=pd.concat([speeches_ds,speeches])\n",
        "        bar()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "speeches_ds.to_csv('data/speeches_termV.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# term5 = pd.read_csv('data/terms/term_5.csv')\n",
        "\n",
        "# for index,r in term5.iterrows():    \n",
        "#     text=getPublicationText(r[\"url\"])\n",
        "#     speeches=get_speeches(text)\n",
        "#     if(index>1):\n",
        "#         break\n",
        "\n",
        "# speeches.head()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "77df4a96c0be8bd891423502f384941e461f81b0e784fdbc8bb136153927d3d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
