{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNCTIONS\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def get_df_from_congress_json(list)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    get_df_from_pubmed_json(list) takes a list of json objects from the congress\n",
        "    and returns a pandas dataframe with the list of documents and the link to\n",
        "    the file in the `url` field.\n",
        "    \"\"\"\n",
        "    temp = []\n",
        "    page_base = 'https://www.congreso.es/busqueda-de-publicaciones?p_p_id=publicaciones&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&_publicaciones_mode=mostrarTextoIntegro&_publicaciones_legislatura={}&_publicaciones_texto=&_publicaciones_id_texto={}'\n",
        "\n",
        "    for s in list:\n",
        "        if(s.startswith('documento')):\n",
        "            temp.append(list[s])\n",
        "\n",
        "    page_no = list['paginaActual']\n",
        "    #doc_len = list['publicaciones_encontradas']\n",
        "    term = list['legislatura']\n",
        "\n",
        "    print(page_no, end='')\n",
        "\n",
        "    df = pd.DataFrame(temp)\n",
        "\n",
        "    df['url'] = df.apply(lambda x: page_base.format(term, (f'{x[\"cve\"]}') if 'cve' in x else (f'{x[\"texto_integro\"][x[\"texto_integro\"].rfind(\"+\")+1:]}') ),axis=1)\n",
        "    df['term']=term\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_term_json(term:str, page:int):\n",
        "  import json\n",
        "  import urllib3\n",
        "  import os\n",
        "  import io\n",
        "\n",
        "  pagequery = \"https://www.congreso.es/publicaciones-organo?p_p_id=publicaciones&p_p_lifecycle=2&p_p_state=normal&p_p_mode=view&p_p_resource_id=filtrarListado&p_p_cacheability=cacheLevelPage&_publicaciones_seccion=Congreso&_publicaciones_descOrg=Pleno-y-Diputacion-Permanente&_publicaciones_publicacion=D\"\n",
        "  http = urllib3.PoolManager()\n",
        "  headers = { 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "              'accept':'application/json, text/javascript, */*; q=0.01'\n",
        "              }\n",
        "\n",
        "  pageform = { \"_publicaciones_legislatura\":term,\n",
        "            \"_publicaciones_comision\":'',\n",
        "            \"_publicaciones_seccion\":'',\n",
        "            \"_publicaciones_fromOrganos\":1,\n",
        "            \"_publicaciones_paginaActual\":page\n",
        "            }\n",
        "\n",
        "  r = http.request_encode_body('POST',pagequery,headers=headers,fields=pageform,encode_multipart=False)\n",
        "  data=r.data.decode('utf-8')\n",
        "  try:\n",
        "    os.makedirs('data/terms',exist_ok=True)\n",
        "    with io.open(file=f\"data/terms/term_{term}_{page}.json\",mode='w') as file:\n",
        "      file.write(data)\n",
        "  except Exception as e:\n",
        "    print(f\"could not write file data/terms/term_{term}_{page}.json: {e}\")\n",
        "\n",
        "  return json.loads(data)\n",
        "\n",
        "def get_all_pages_for_a_term(term:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    get_all_pages_for_a_term(term:int)->pd.DataFrame takes a term (legislature)\n",
        "    and returns a dataframe with all the documents in that term.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    \n",
        "    term_ds = None\n",
        "    nextPage = 1\n",
        "    while nextPage:\n",
        "      list = get_term_json(term, nextPage)\n",
        "      ds = get_df_from_congress_json(list)\n",
        "      if term_ds is None:\n",
        "        term_ds = ds\n",
        "      else:\n",
        "        term_ds = pd.concat([term_ds,ds])\n",
        "      lastDoc=int(list['paginacion']['docs_fin'])\n",
        "      totalDocs=int(list['publicaciones_encontradas'])\n",
        "      if(lastDoc<totalDocs):\n",
        "        nextPage+=1\n",
        "        print('.',end=\"\")\n",
        "      else:\n",
        "        nextPage=0\n",
        "    try:\n",
        "      os.makedirs('data/terms',exist_ok=True)\n",
        "      term_ds.to_csv(f\"data/terms/term_{term}.csv\",index=False)\n",
        "    except Exception as e:\n",
        "      print(f\"could not write file data/terms/term_{term}.csv: {e}\")\n",
        "    return term_ds\n",
        "\n",
        "def get_all_terms(max:int=14)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    get_all_terms(max:int=14)->pd.DataFrame takes a maximum term and returns all\n",
        "    the documents in all the terms (from the 5th that is the one that starts\n",
        "    having raw text data).\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    full_ds = None\n",
        "    for i in range(5,max+1):\n",
        "      print(f\"Getting term {i}...\",end=\"\")\n",
        "      ds = get_all_pages_for_a_term(i)\n",
        "      if full_ds is None:\n",
        "        full_ds = ds\n",
        "      else:\n",
        "        full_ds = pd.concat([full_ds,ds])\n",
        "      print(f\"Done. {len(full_ds)} documents.\")\n",
        "    return full_ds\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Constants\n",
        "\n",
        "max_term=14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting term 5...1.2.3.4.5.6.7.8.9.10Done. 197 documents.\n",
            "Getting term 6...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15Done. 483 documents.\n",
            "Getting term 7...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16Done. 793 documents.\n",
            "Getting term 8...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16Done. 1108 documents.\n",
            "Getting term 9...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15Done. 1390 documents.\n",
            "Getting term 10...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16Done. 1705 documents.\n",
            "Getting term 11...1Done. 1720 documents.\n",
            "Getting term 12...1.2.3.4.5.6.7.8.9.10Done. 1905 documents.\n",
            "Getting term 13...1Done. 1920 documents.\n",
            "Getting term 14...1.2.3.4.5.6.7.8.9.10.11.12Done. 2141 documents.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "full_ds = get_all_terms(max_term)\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "full_ds.to_csv(f\"data/full_term_up_to_{max_term}.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "77df4a96c0be8bd891423502f384941e461f81b0e784fdbc8bb136153927d3d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
