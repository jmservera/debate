{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/docs/datasets/v1.11.0/add_dataset.html\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "\r\n",
        "# prepare dataset\r\n",
        "df=pd.read_csv('../rajoy.csv')"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670800946468
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'¿CREE QUE LA POLÍTICA DE SU GOBIERNO TRANSMITE CONFIANZA EN LA ECONOMÍA ESPAÑOLA?'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670772236948
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# todo move this cleanup to the prepare-data\r\n",
        "#df[] # get texts with at least 31 words (30 spaces in between)\r\n",
        "df=df[df['Text'].str.count(' ')>30]\r\n",
        "df=df[~df.Text.str.contains(\"mero de expediente \\d+/\\d+\", regex=True)]\r\n",
        "df.info()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1821 entries, 1 to 2122\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Name              1821 non-null   object \n 1   Text              1821 non-null   object \n 2   Date              1821 non-null   object \n 3   Term              1821 non-null   object \n 4   Title             1195 non-null   object \n 5   infos             1576 non-null   object \n 6   TextLen           1821 non-null   float64\n 7   Data              11 non-null     object \n 8   InterventionType  11 non-null     object \n 9   Positive          1821 non-null   int64  \n 10  Negative          1821 non-null   int64  \n 11  Surprise          1821 non-null   int64  \ndtypes: float64(1), int64(3), object(8)\nmemory usage: 184.9+ KB\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670800949882
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_splitter(df: pd.DataFrame)->pd.DataFrame:\r\n",
        "    \"\"\"\r\n",
        "    This method can be used to split sentences and create a new DataFrame from it. \r\n",
        "    \"\"\"\r\n",
        "    sentences=df[\"Text\"].to_list()\r\n",
        "\r\n",
        "    # splits sentences using marks\r\n",
        "    split_stc=[re.split('(\\.\\s|\\?|\\!|;)',str(s)) for s in sentences]\r\n",
        "    stcs=[]\r\n",
        "    # joins with the mark again\r\n",
        "    for l in split_stc:\r\n",
        "        liter=iter(l)\r\n",
        "        stcs.append([h + next(liter,'') for h in liter])\r\n",
        "\r\n",
        "    # unflatten and remove sentences shorter than 10 chars\r\n",
        "    sentences=[n.strip() for s in stcs for n in s if len(n.strip())>10]\r\n",
        "    return pd.DataFrame(sentences, columns=[\"Text\"])\r\n",
        "\r\n",
        "\r\n",
        "# get the last 20 words of the test dataset do the generator\r\n",
        "df['end_Text']=df['Text'].str.split().str[-20:].apply(' '.join)\r\n",
        "df['Text']=df['Text'].str.split().str[:-20].apply(' '.join)\r\n",
        "\r\n",
        "# Creating a dataframe with 50%\r\n",
        "# values of original dataframe\r\n",
        "part_85 = df.sample(frac = 0.85)\r\n",
        " \r\n",
        "# Creating dataframe with\r\n",
        "# rest of the 50% values\r\n",
        "rest_part_85 = df.drop(part_85.index)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670800967461
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset,DatasetDict\r\n",
        "\r\n",
        "ds_train=Dataset.from_pandas(part_85)\r\n",
        "ds_test=Dataset.from_pandas(rest_part_85)\r\n",
        "\r\n",
        "\r\n",
        "#ds=Dataset.from_dict({'text':sentences})\r\n",
        "ds = DatasetDict()\r\n",
        "ds['train'] = ds_train\r\n",
        "ds['test'] = ds_test\r\n",
        "\r\n",
        "ds_train.to_json('../rajoy_train.jsonl')\r\n",
        "ds_test.to_json('../rajoy_test.jsonl')\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "923c23634d354e52900f557c031f858b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e20554b042d49a99ff12ee97c1a13d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "1514091"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670800977551
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds=load_dataset('.',data_files={'train': ['../rajoy_train.jsonl'],\r\n",
        "                             'test':['../rajoy_test.jsonl']})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using custom data configuration .-178e1b280ffe2377\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading and preparing dataset json/. to /home/azureuser/.cache/huggingface/datasets/json/.-178e1b280ffe2377/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d257287054e6421188cb53d45f75297c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c92b919c2a894785892b80c50fb836d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e490f2ba04543e2983a3509c88c8125"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc27f83679e349d890f3029f72207810"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset json downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/json/.-178e1b280ffe2377/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5d76017ff124d85994aa01231f42bf8"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670541328095
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "# TODO: Address all TODOs and remove all explanatory comments\r\n",
        "\"\"\"The Spanish politician debates dataset\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import os\r\n",
        "\r\n",
        "import datasets\r\n",
        "\r\n",
        "\r\n",
        "# TODO: Add BibTeX citation\r\n",
        "# Find for instance the citation on arxiv or on the dataset repo/website\r\n",
        "_CITATION = \"\"\"\\\r\n",
        "@InProceedings{huggingface:dataset,\r\n",
        "title = {A Spanish politician debates dataset},\r\n",
        "author={Juan Manuel Servera Bondroit},\r\n",
        "year={2022}\r\n",
        "}\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# TODO: Add description of the dataset here\r\n",
        "# You can copy an official description\r\n",
        "_DESCRIPTION = \"\"\"\\\r\n",
        "This dataset is extracted from the www.congreso.es website, where you can download all the political debates for the last 10 terms in the form of text files.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# TODO: Add a link to an official homepage for the dataset here\r\n",
        "_HOMEPAGE = \"\"\r\n",
        "\r\n",
        "# TODO: Add the licence for the dataset here if you can find it\r\n",
        "_LICENSE = \"Apache-2.0\"\r\n",
        "\r\n",
        "# TODO: Add link to the official dataset URLs here\r\n",
        "# The HuggingFace Datasets library doesn't host the datasets but only points to the original files.\r\n",
        "# This can be an arbitrary nested dict/list of URLs (see below in `_split_generators` method)\r\n",
        "_URLS = {\r\n",
        "    \"first_domain\": \"https://huggingface.co/great-new-dataset-first_domain.zip\",\r\n",
        "    \"second_domain\": \"https://huggingface.co/great-new-dataset-second_domain.zip\",\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# TODO: Name of the dataset usually matches the script name with CamelCase instead of snake_case\r\n",
        "class SpanishDebatesDataset(datasets.GeneratorBasedBuilder):\r\n",
        "    \"\"\"The Spanish politician debates dataset.\"\"\"\r\n",
        "\r\n",
        "    VERSION = datasets.Version(\"0.1.0\")\r\n",
        "\r\n",
        "    # This is an example of a dataset with multiple configurations.\r\n",
        "    # If you don't want/need to define several sub-sets in your dataset,\r\n",
        "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\r\n",
        "\r\n",
        "    # If you need to make complex sub-parts in the datasets with configurable options\r\n",
        "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\r\n",
        "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\r\n",
        "\r\n",
        "    # You will be able to load one or the other configurations in the following list with\r\n",
        "    # data = datasets.load_dataset('my_dataset', 'first_domain')\r\n",
        "    # data = datasets.load_dataset('my_dataset', 'second_domain')\r\n",
        "    # BUILDER_CONFIGS = [\r\n",
        "    #     datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),\r\n",
        "    #     datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),\r\n",
        "    # ]\r\n",
        "\r\n",
        "    # DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\r\n",
        "\r\n",
        "    def _info(self):\r\n",
        "        features = datasets.Features(\r\n",
        "                {\r\n",
        "                    \"sentence\": datasets.Value(\"string\")\r\n",
        "                }\r\n",
        "        )\r\n",
        "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\r\n",
        "        # if self.config.name == \"first_domain\":  # This is the name of the configuration selected in BUILDER_CONFIGS above\r\n",
        "            \r\n",
        "        #     )\r\n",
        "        # else:  # This is an example to show how to have different features for \"first_domain\" and \"second_domain\"\r\n",
        "        #     features = datasets.Features(\r\n",
        "        #         {\r\n",
        "        #             \"sentence\": datasets.Value(\"string\"),\r\n",
        "        #             \"option2\": datasets.Value(\"string\"),\r\n",
        "        #             \"second_domain_answer\": datasets.Value(\"string\")\r\n",
        "        #             # These are the features of your dataset like images, labels ...\r\n",
        "        #         }\r\n",
        "        #     )\r\n",
        "        return datasets.DatasetInfo(\r\n",
        "            # This is the description that will appear on the datasets page.\r\n",
        "            description=_DESCRIPTION,\r\n",
        "            # This defines the different columns of the dataset and their types\r\n",
        "            features=features,  # Here we define them above because they are different between the two configurations\r\n",
        "            # If there's a common (input, target) tuple from the features, uncomment supervised_keys line below and\r\n",
        "            # specify them. They'll be used if as_supervised=True in builder.as_dataset.\r\n",
        "            # supervised_keys=(\"sentence\", \"label\"),\r\n",
        "            # Homepage of the dataset for documentation\r\n",
        "            homepage=_HOMEPAGE,\r\n",
        "            # License for the dataset if available\r\n",
        "            license=_LICENSE,\r\n",
        "            # Citation for the dataset\r\n",
        "            citation=_CITATION,\r\n",
        "        )\r\n",
        "\r\n",
        "    def _split_generators(self, dl_manager):\r\n",
        "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\r\n",
        "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\r\n",
        "\r\n",
        "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\r\n",
        "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\r\n",
        "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\r\n",
        "        urls = _URLS[self.config.name]\r\n",
        "        data_dir = dl_manager.download_and_extract(urls)\r\n",
        "        return [\r\n",
        "            datasets.SplitGenerator(\r\n",
        "                name=datasets.Split.TRAIN,\r\n",
        "                # These kwargs will be passed to _generate_examples\r\n",
        "                gen_kwargs={\r\n",
        "                    \"filepath\": os.path.join(data_dir, \"train.jsonl\"),\r\n",
        "                    \"split\": \"train\",\r\n",
        "                },\r\n",
        "            ),\r\n",
        "            datasets.SplitGenerator(\r\n",
        "                name=datasets.Split.VALIDATION,\r\n",
        "                # These kwargs will be passed to _generate_examples\r\n",
        "                gen_kwargs={\r\n",
        "                    \"filepath\": os.path.join(data_dir, \"dev.jsonl\"),\r\n",
        "                    \"split\": \"dev\",\r\n",
        "                },\r\n",
        "            ),\r\n",
        "            datasets.SplitGenerator(\r\n",
        "                name=datasets.Split.TEST,\r\n",
        "                # These kwargs will be passed to _generate_examples\r\n",
        "                gen_kwargs={\r\n",
        "                    \"filepath\": os.path.join(data_dir, \"test.jsonl\"),\r\n",
        "                    \"split\": \"test\"\r\n",
        "                },\r\n",
        "            ),\r\n",
        "        ]\r\n",
        "\r\n",
        "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\r\n",
        "    def _generate_examples(self, filepath, split):\r\n",
        "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\r\n",
        "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\r\n",
        "        with open(filepath, encoding=\"utf-8\") as f:\r\n",
        "            for key, row in enumerate(f):\r\n",
        "                data = json.loads(row)\r\n",
        "                yield key, {\r\n",
        "                        \"sentence\": data[\"sentence\"],\r\n",
        "                    }\r\n",
        "                # if self.config.name == \"first_domain\":\r\n",
        "                #     # Yields examples as (key, example) tuples\r\n",
        "                #     yield key, {\r\n",
        "                #         \"sentence\": data[\"sentence\"],\r\n",
        "                #         \"option1\": data[\"option1\"],\r\n",
        "                #         \"answer\": \"\" if split == \"test\" else data[\"answer\"],\r\n",
        "                #     }\r\n",
        "                # else:\r\n",
        "                #     yield key, {\r\n",
        "                #         \"sentence\": data[\"sentence\"],\r\n",
        "                #         \"option2\": data[\"option2\"],\r\n",
        "                #         \"second_domain_answer\": \"\" if split == \"test\" else data[\"second_domain_answer\"],\r\n",
        "                #     }"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670541328202
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}