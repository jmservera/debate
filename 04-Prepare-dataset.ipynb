{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/docs/datasets/v1.11.0/add_dataset.html\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "\r\n",
        "# prepare dataset\r\n",
        "df=pd.read_csv('../rajoy.csv')\r\n",
        "\r\n",
        "# todo move this cleanup to the prepare-data\r\n",
        "df=df[df.Text.str.len()>10]\r\n",
        "df=df[~df.Text.str.contains(\"mero de expediente \\d+/\\d+\", regex=True)]\r\n",
        "df.info()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1896 entries, 1 to 2058\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    1896 non-null   object\n 1   Text    1896 non-null   object\n 2   Date    1896 non-null   int64 \n 3   Term    1896 non-null   object\n 4   Title   1205 non-null   object\ndtypes: int64(1), object(4)\nmemory usage: 88.9+ KB\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1668791359064
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=df[\"Text\"].to_list()\r\n",
        "\r\n",
        "# splits sentences using marks\r\n",
        "split_stc=[re.split('(\\.\\s|\\?|\\!|;)',str(s)) for s in sentences]\r\n",
        "stcs=[]\r\n",
        "# joins with the mark again\r\n",
        "for l in split_stc:\r\n",
        "    liter=iter(l)\r\n",
        "    stcs.append([h + next(liter,'') for h in liter])\r\n",
        "\r\n",
        "# unflatten and remove sentences shorter than 10 chars\r\n",
        "sentences=[n.strip() for s in stcs for n in s if len(n.strip())>10]\r\n",
        "df=pd.DataFrame(sentences, columns=[\"Text\"])\r\n",
        "\r\n",
        "# Creating a dataframe with 50%\r\n",
        "# values of original dataframe\r\n",
        "part_55 = df.sample(frac = 0.55).reset_index()\r\n",
        " \r\n",
        "# Creating dataframe with\r\n",
        "# rest of the 50% values\r\n",
        "rest_part_55 = df.drop(part_55.index).reset_index()"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1668791593940
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset,DatasetDict\r\n",
        "\r\n",
        "ds_train=Dataset.from_pandas(part_55)\r\n",
        "ds_test=Dataset.from_pandas(rest_part_55)\r\n",
        "#ds=Dataset.from_dict({'text':sentences})\r\n",
        "ds = DatasetDict()\r\n",
        "ds['train'] = ds_train\r\n",
        "ds['test'] = ds_test\r\n",
        "\r\n",
        "ds_train.to_json('../rajoy_train.jsonl')\r\n",
        "ds_test.to_json('../rajoy_test.jsonl')\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5dd84f8dd1d45ed9bce1b293fda3c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2534d9735a943419a12147e02388c6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "4121781"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1668791826887
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds=load_dataset('.',data_files={'train': ['../rajoy_train.jsonl'],\r\n",
        "                             'test':['../rajoy_test.jsonl']})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using custom data configuration .-b1c978082bff5d3c\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d9b501e6f114bcea01cc564944689a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df2b8b9d78d448d0881aa09fd2bf5b5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce5cbad48d614c27955ad79107be7152"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0362933770c6479b844c42b6da2e5e2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a29bac4e88254bce80cdbc4dd13d279f"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1668792021539
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['index', 'Text'],\n        num_rows: 28962\n    })\n    test: Dataset({\n        features: ['index', 'Text'],\n        num_rows: 23697\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1668792030014
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "# TODO: Address all TODOs and remove all explanatory comments\r\n",
        "\"\"\"The Spanish politician debates dataset\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import os\r\n",
        "\r\n",
        "import datasets\r\n",
        "\r\n",
        "\r\n",
        "# TODO: Add BibTeX citation\r\n",
        "# Find for instance the citation on arxiv or on the dataset repo/website\r\n",
        "_CITATION = \"\"\"\\\r\n",
        "@InProceedings{huggingface:dataset,\r\n",
        "title = {A Spanish politician debates dataset},\r\n",
        "author={Juan Manuel Servera Bondroit},\r\n",
        "year={2022}\r\n",
        "}\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# TODO: Add description of the dataset here\r\n",
        "# You can copy an official description\r\n",
        "_DESCRIPTION = \"\"\"\\\r\n",
        "This dataset is extracted from the www.congreso.es website, where you can download all the political debates for the last 10 terms in the form of text files.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# TODO: Add a link to an official homepage for the dataset here\r\n",
        "_HOMEPAGE = \"\"\r\n",
        "\r\n",
        "# TODO: Add the licence for the dataset here if you can find it\r\n",
        "_LICENSE = \"Apache-2.0\"\r\n",
        "\r\n",
        "# TODO: Add link to the official dataset URLs here\r\n",
        "# The HuggingFace Datasets library doesn't host the datasets but only points to the original files.\r\n",
        "# This can be an arbitrary nested dict/list of URLs (see below in `_split_generators` method)\r\n",
        "_URLS = {\r\n",
        "    \"first_domain\": \"https://huggingface.co/great-new-dataset-first_domain.zip\",\r\n",
        "    \"second_domain\": \"https://huggingface.co/great-new-dataset-second_domain.zip\",\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# TODO: Name of the dataset usually matches the script name with CamelCase instead of snake_case\r\n",
        "class SpanishDebatesDataset(datasets.GeneratorBasedBuilder):\r\n",
        "    \"\"\"The Spanish politician debates dataset.\"\"\"\r\n",
        "\r\n",
        "    VERSION = datasets.Version(\"0.1.0\")\r\n",
        "\r\n",
        "    # This is an example of a dataset with multiple configurations.\r\n",
        "    # If you don't want/need to define several sub-sets in your dataset,\r\n",
        "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\r\n",
        "\r\n",
        "    # If you need to make complex sub-parts in the datasets with configurable options\r\n",
        "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\r\n",
        "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\r\n",
        "\r\n",
        "    # You will be able to load one or the other configurations in the following list with\r\n",
        "    # data = datasets.load_dataset('my_dataset', 'first_domain')\r\n",
        "    # data = datasets.load_dataset('my_dataset', 'second_domain')\r\n",
        "    # BUILDER_CONFIGS = [\r\n",
        "    #     datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),\r\n",
        "    #     datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),\r\n",
        "    # ]\r\n",
        "\r\n",
        "    # DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\r\n",
        "\r\n",
        "    def _info(self):\r\n",
        "        features = datasets.Features(\r\n",
        "                {\r\n",
        "                    \"sentence\": datasets.Value(\"string\")\r\n",
        "                }\r\n",
        "        )\r\n",
        "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\r\n",
        "        # if self.config.name == \"first_domain\":  # This is the name of the configuration selected in BUILDER_CONFIGS above\r\n",
        "            \r\n",
        "        #     )\r\n",
        "        # else:  # This is an example to show how to have different features for \"first_domain\" and \"second_domain\"\r\n",
        "        #     features = datasets.Features(\r\n",
        "        #         {\r\n",
        "        #             \"sentence\": datasets.Value(\"string\"),\r\n",
        "        #             \"option2\": datasets.Value(\"string\"),\r\n",
        "        #             \"second_domain_answer\": datasets.Value(\"string\")\r\n",
        "        #             # These are the features of your dataset like images, labels ...\r\n",
        "        #         }\r\n",
        "        #     )\r\n",
        "        return datasets.DatasetInfo(\r\n",
        "            # This is the description that will appear on the datasets page.\r\n",
        "            description=_DESCRIPTION,\r\n",
        "            # This defines the different columns of the dataset and their types\r\n",
        "            features=features,  # Here we define them above because they are different between the two configurations\r\n",
        "            # If there's a common (input, target) tuple from the features, uncomment supervised_keys line below and\r\n",
        "            # specify them. They'll be used if as_supervised=True in builder.as_dataset.\r\n",
        "            # supervised_keys=(\"sentence\", \"label\"),\r\n",
        "            # Homepage of the dataset for documentation\r\n",
        "            homepage=_HOMEPAGE,\r\n",
        "            # License for the dataset if available\r\n",
        "            license=_LICENSE,\r\n",
        "            # Citation for the dataset\r\n",
        "            citation=_CITATION,\r\n",
        "        )\r\n",
        "\r\n",
        "    def _split_generators(self, dl_manager):\r\n",
        "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\r\n",
        "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\r\n",
        "\r\n",
        "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\r\n",
        "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\r\n",
        "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\r\n",
        "        urls = _URLS[self.config.name]\r\n",
        "        data_dir = dl_manager.download_and_extract(urls)\r\n",
        "        return [\r\n",
        "            datasets.SplitGenerator(\r\n",
        "                name=datasets.Split.TRAIN,\r\n",
        "                # These kwargs will be passed to _generate_examples\r\n",
        "                gen_kwargs={\r\n",
        "                    \"filepath\": os.path.join(data_dir, \"train.jsonl\"),\r\n",
        "                    \"split\": \"train\",\r\n",
        "                },\r\n",
        "            ),\r\n",
        "            datasets.SplitGenerator(\r\n",
        "                name=datasets.Split.VALIDATION,\r\n",
        "                # These kwargs will be passed to _generate_examples\r\n",
        "                gen_kwargs={\r\n",
        "                    \"filepath\": os.path.join(data_dir, \"dev.jsonl\"),\r\n",
        "                    \"split\": \"dev\",\r\n",
        "                },\r\n",
        "            ),\r\n",
        "            datasets.SplitGenerator(\r\n",
        "                name=datasets.Split.TEST,\r\n",
        "                # These kwargs will be passed to _generate_examples\r\n",
        "                gen_kwargs={\r\n",
        "                    \"filepath\": os.path.join(data_dir, \"test.jsonl\"),\r\n",
        "                    \"split\": \"test\"\r\n",
        "                },\r\n",
        "            ),\r\n",
        "        ]\r\n",
        "\r\n",
        "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\r\n",
        "    def _generate_examples(self, filepath, split):\r\n",
        "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\r\n",
        "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\r\n",
        "        with open(filepath, encoding=\"utf-8\") as f:\r\n",
        "            for key, row in enumerate(f):\r\n",
        "                data = json.loads(row)\r\n",
        "                yield key, {\r\n",
        "                        \"sentence\": data[\"sentence\"],\r\n",
        "                    }\r\n",
        "                # if self.config.name == \"first_domain\":\r\n",
        "                #     # Yields examples as (key, example) tuples\r\n",
        "                #     yield key, {\r\n",
        "                #         \"sentence\": data[\"sentence\"],\r\n",
        "                #         \"option1\": data[\"option1\"],\r\n",
        "                #         \"answer\": \"\" if split == \"test\" else data[\"answer\"],\r\n",
        "                #     }\r\n",
        "                # else:\r\n",
        "                #     yield key, {\r\n",
        "                #         \"sentence\": data[\"sentence\"],\r\n",
        "                #         \"option2\": data[\"option2\"],\r\n",
        "                #         \"second_domain_answer\": \"\" if split == \"test\" else data[\"second_domain_answer\"],\r\n",
        "                #     }"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1668790586824
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}