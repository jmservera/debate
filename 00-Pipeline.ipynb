{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use this notebook to runn all the notebooks as a pipeline\r\n",
        "\r\n",
        "Building this dataset takes multiple steps:\r\n",
        "\r\n",
        "1. First we need to gather the list of pages we can use for building it, there's a search engine that returns a `json` file with the list of pages.\r\n",
        "2. We import all the HTML and do an initial parsing to extract the debates and name of the politician for each speech.\r\n",
        "3. Then we build a huge dataset with all the imported data, and enhance it by extracting the polititian title if any, extract sentiment and cleanup data.\r\n",
        "4. Once we have the full dataset, we prepare it for publishing it on Hugging Face and make it compatible with the Transformers platform.\r\n",
        "\r\n",
        "Each of this steps corresponds to a Notebook, you can run them all with this pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: run search.\r\n",
        "\r\n",
        "The result is a bunch of json files pointing to the real HTML files, into the ./data/terms folder. We also store partial csv and a full one."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./01-CDLD-Get-full-term-in-office.ipynb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Getting term 5...1.2.3.4.5.6.7.8.9.10Done. 197 documents.\nGetting term 6...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15Done. 483 documents.\nGetting term 7...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16Done. 793 documents.\nGetting term 8...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16Done. 1108 documents.\nGetting term 9...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15Done. 1390 documents.\nGetting term 10...1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16Done. 1705 documents.\nGetting term 11...1Done. 1720 documents.\nGetting term 12...1.2.3.4.5.6.7.8.9.10Done. 1905 documents.\nGetting term 13...1Done. 1920 documents.\nGetting term 14...1.2.3.4.5.6.7.8.9.10.11.12Done. 2150 documents.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670849778667
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import all the debates from HTML\r\n",
        "\r\n",
        "The result is stored as a csv file for each debate in ./data/debates. Those files will contain all the speeches with the name of the politician giving the speech.\r\n",
        "We also store the HTML pages into ./data/pagecache to avoid downloading all the content each time."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./02-CDLD-import.ipynb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: alive_progress in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.4.1)\nRequirement already satisfied: grapheme==0.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (0.6.0)\nRequirement already satisfied: about-time==3.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (3.1.1)\nNote: you may need to restart the kernel to use updated packages.\nimporting Term X |████████████████████████████████████████| 315/315 [100%] in 2:10.6 (2.41/s)                           \nimporting Term XI |████████████████████████████████████████| 15/15 [100%] in 3.4s (4.44/s)                              \nimporting Term XII |████████████████████████████████████████| 185/185 [100%] in 1:16.1 (2.43/s)                         \nimporting Term XIII |████████████████████████████████████████| 15/15 [100%] in 4.7s (3.18/s)                            \non 221: Error: PRESIDENT* not found                                                                                     \non 222: Error: PRESIDENT* not found                                                                                     \non 223: Error: PRESIDENT* not found                                                                                     \non 224: Error: PRESIDENT* not found                                                                                     \non 226: Error: PRESIDENT* not found                                                                                     \non 227: Error: PRESIDENT* not found                                                                                     \non 229: Error: PRESIDENT* not found                                                                                     \nimporting Term XIV |████████████████████████████████████████| 230/230 [100%] in 1:44.6 (2.20/s)                         \nimporting Term V |████████████████████████████████████████| 197/197 [100%] in 1:22.0 (2.40/s)                           \nimporting Term VI |████████████████████████████████████████| 286/286 [100%] in 2:06.2 (2.27/s)                          \nimporting Term VII |████████████████████████████████████████| 310/310 [100%] in 2:11.2 (2.36/s)                         \nimporting Term VIII |████████████████████████████████████████| 315/315 [100%] in 2:02.2 (2.58/s)                        \nimporting Term IX |████████████████████████████████████████| 282/282 [100%] in 1:45.6 (2.67/s)                          \n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare and clean the dataset.\r\n",
        "\r\n",
        "Data enhancement by removing paranthesized text and extracting data from these items like politician title, related decrees or reactions to have a sentiment score."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./03-Prepare-Data.ipynb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "             TextLen       Positive       Negative       Surprise\ncount  357970.000000  358067.000000  358067.000000  358067.000000\nmean     1468.588885       0.296344       0.125094       0.002058\nstd      3119.074758       1.061502       0.617428       0.052462\nmin         0.000000       0.000000       0.000000       0.000000\n25%        76.000000       0.000000       0.000000       0.000000\n50%       200.000000       0.000000       0.000000       0.000000\n75%      1355.000000       0.000000       0.000000       0.000000\nmax    101263.000000     105.000000      46.000000       7.000000\ndebates written to parquet file.\nExported Zapater & Rajoy\n            TextLen     Positive     Negative     Surprise\ncount   2123.000000  2123.000000  2123.000000  2123.000000\nmean    3701.005652     2.113990     0.740462     0.026849\nstd     7683.510591     4.321462     1.791697     0.188588\nmin        2.000000     0.000000     0.000000     0.000000\n25%      517.000000     0.000000     0.000000     0.000000\n50%     1268.000000     1.000000     0.000000     0.000000\n75%     2211.000000     2.000000     1.000000     0.000000\nmax    74996.000000    48.000000    23.000000     3.000000\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: NLP dataset preparation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./04-Prepare-dataset.ipynb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading and preparing dataset json/. to /home/azureuser/.cache/huggingface/datasets/json/.-ec277ea29f9ac72a/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\nDataset json downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/json/.-ec277ea29f9ac72a/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1821 entries, 1 to 2122\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Name              1821 non-null   object \n 1   Text              1821 non-null   object \n 2   Date              1821 non-null   object \n 3   Term              1821 non-null   object \n 4   Title             1195 non-null   object \n 5   infos             1576 non-null   object \n 6   TextLen           1821 non-null   float64\n 7   Data              11 non-null     object \n 8   InterventionType  11 non-null     object \n 9   Positive          1821 non-null   int64  \n 10  Negative          1821 non-null   int64  \n 11  Surprise          1821 non-null   int64  \ndtypes: float64(1), int64(3), object(8)\nmemory usage: 184.9+ KB\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67eeb346544b4d55a26624b4e51e769e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edb48d82fa184a238724efc06934e102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using custom data configuration .-7d843941c9c1b74a\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "044507b2587946559364dc5395fa4673"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f838dffa20041c48c4c1987d65d2a8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5a3a9c1af1840abb80771a54b25521b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca2ccd1676324362bf663724c2dbf6fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8bb9eb4c2304981aa2147de28c7212e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}