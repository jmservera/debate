{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use this notebook to runn all the notebooks as a pipeline\r\n",
        "\r\n",
        "Building this dataset takes multiple steps:\r\n",
        "\r\n",
        "1. First we need to gather the list of pages we can use for building it, there's a search engine that returns a `json` file with the list of pages.\r\n",
        "2. We import all the HTML and do an initial parsing to extract the debates and name of the politician for each speech.\r\n",
        "3. Then we build a huge dataset with all the imported data, and enhance it by extracting the polititian title if any, extract sentiment and cleanup data.\r\n",
        "4. Once we have the full dataset, we prepare it for publishing it on Hugging Face and make it compatible with the Transformers platform.\r\n",
        "\r\n",
        "Each of this steps corresponds to a Notebook, you can run them all with this pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: run search.\r\n",
        "\r\n",
        "The result is a bunch of json files pointing to the real HTML files, into the ./data/terms folder. We also store partial csv and a full one."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %run ./01-CDLD-Get-full-term-in-office.ipynb"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670849778667
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import all the debates from HTML\r\n",
        "\r\n",
        "The result is stored as a csv file for each debate in ./data/debates. Those files will contain all the speeches with the name of the politician giving the speech.\r\n",
        "We also store the HTML pages into ./data/pagecache to avoid downloading all the content each time."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./02-CDLD-import.ipynb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: alive_progress in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.4.1)\nRequirement already satisfied: about-time==3.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (3.1.1)\nRequirement already satisfied: grapheme==0.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (0.6.0)\nNote: you may need to restart the kernel to use updated packages.\nimporting Term X |████████████████████████████████████████| 315/315 [100%] in 2:13.2 (2.37/s)                           \nimporting Term XI |████████████████████████████████████████| 15/15 [100%] in 3.6s (4.16/s)                              \nimporting Term XII |████████████████████████████████████████| 185/185 [100%] in 1:17.7 (2.38/s)                         \nimporting Term XIII |████████████████████████████████████████| 15/15 [100%] in 4.8s (3.10/s)                            \nimporting Term XIV |████████████████████████████████████████| 221/221 [100%] in 1:42.7 (2.15/s)                         \nimporting Term V |████████████████████████████████████████| 197/197 [100%] in 1:24.0 (2.34/s)                           \nimporting Term VI |████████████████████████████████████████| 286/286 [100%] in 2:06.8 (2.26/s)                          \nimporting Term VII |████████████████████████████████████████| 310/310 [100%] in 2:08.1 (2.42/s)                         \nimporting Term VIII |████████████████████████████████████████| 315/315 [100%] in 1:58.6 (2.66/s)                        \nimporting Term IX |████████████████████████████████████████| 282/282 [100%] in 1:46.3 (2.65/s)                          \n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare and clean the dataset.\r\n",
        "\r\n",
        "Data enhancement by removing paranthesized text and extracting data from these items like politician title, related decrees or reactions to have a sentiment score."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./03-Prepare-Data.ipynb"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: NLP dataset preparation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./04-Prepare-dataset.ipynb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1821 entries, 1 to 2122\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Name              1821 non-null   object \n 1   Text              1821 non-null   object \n 2   Date              1821 non-null   object \n 3   Term              1821 non-null   object \n 4   Title             1195 non-null   object \n 5   infos             1576 non-null   object \n 6   TextLen           1821 non-null   float64\n 7   Data              11 non-null     object \n 8   InterventionType  11 non-null     object \n 9   Positive          1821 non-null   int64  \n 10  Negative          1821 non-null   int64  \n 11  Surprise          1821 non-null   int64  \ndtypes: float64(1), int64(3), object(8)\nmemory usage: 184.9+ KB\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfe0ae5c09784b5183dff99e8ae7a398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9443eac6735432d958f7c006af5272b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading and preparing dataset json/. to /home/azureuser/.cache/huggingface/datasets/json/.-0452fd0be2cf681c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35a3225ada5244f790dba766cd700397"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3a0e46ad77345bc8b018da0c2446f3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4722f1c655ee49c3a02e501f6a31c0a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 tables [00:00, ? tables/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc79dea408c24a578bd506e30b61a9fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset json downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/json/.-0452fd0be2cf681c/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5. Subsequent calls will reuse this data.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "709ff71ddb734510a83d71aa411a41f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using custom data configuration .-0452fd0be2cf681c\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}