{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 02: Data import from the \"Congreso de los Diputados\" website\n",
        "\n",
        "This process is focused on extracting the political debates that happened in the congress and tag every intervention with the name of the politician. The main objective is to create a corpora for political profiles that can be used to train an ML Transformer.\n",
        "\n",
        "It relies on the previous step that downloaded a set of links to all the pages for each term to get the HTML pages that contain all the debates that happened in the Spanish Congress.\n",
        "\n",
        "After this process we will have all the terms debates extracted from the web pages into the `./data/debates` folder in `csv` format, one entry for every speech. Speeches are detected when an ALL CAPS name starts appears, this is the format the congress curators use to indicate that a person (can be the name or the title) is starting to speak. The process also strips some noise like the page numbers, internal scripts or special characters not used for the text itself.\n",
        "\n",
        "Any error to this process will be stored in the `./data/errors` page for debug purposes."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installers, we use the alive_progress bar to have a nice progress view during the import\n",
        "%pip install alive_progress"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: alive_progress in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.4.1)\nRequirement already satisfied: about-time==3.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (3.1.1)\nRequirement already satisfied: grapheme==0.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from alive_progress) (0.6.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1670795114753
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Imports\n",
        "from html.parser import HTMLParser\n",
        "import re\n",
        "import pandas as pd\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1670795115046
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "\n",
        "\"\"\"\n",
        "A simple parser to extract all the text of a publication from the body.\n",
        "It removes any internal script and removes special characters with the str.strip function.\n",
        "It also gets rid of pagination (Página nnn)\n",
        "\"\"\"\n",
        "class PublicationParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        #Initializing lists\n",
        "        self.lsStartTags = list()\n",
        "        self.lsEndTags = list()\n",
        "        self.lsStartEndTags = list()\n",
        "        self.lsComments = list()\n",
        "        self.lsData=list()\n",
        "        # Indicates when we are inside the body tag\n",
        "        self.inBody=False\n",
        "        # Marker for scripts\n",
        "        self.inScript=False\n",
        "\n",
        "    #HTML Parser Methods\n",
        "    def handle_starttag(self, startTag, attrs):\n",
        "        self.lsStartTags.append(startTag)\n",
        "        if(startTag==\"body\"):\n",
        "            self.inBody=True\n",
        "        if(startTag==\"script\"):\n",
        "            self.inScript=True\n",
        "\n",
        "    def handle_endtag(self, endTag):\n",
        "        self.lsEndTags.append(endTag)\n",
        "        if(endTag==\"body\"):\n",
        "            self.inBody=False\n",
        "        elif(endTag==\"script\"):\n",
        "            self.inScript=False\n",
        "\n",
        "    def handle_startendtag(self,startendTag, attrs):\n",
        "       self.lsStartEndTags.append(startendTag)\n",
        "\n",
        "    def handle_comment(self,data):\n",
        "       self.lsComments.append(data)\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        if(self.inBody and not self.inScript and data!=''):\n",
        "            if(not (data.startswith('Página ') or data.startswith('(Página') )):\n",
        "                self.lsData.append(data.strip())\n",
        "\n",
        "           \n"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1670796357240
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main body extractor\n",
        "\n",
        "Gets the body and finds the start of the debate by looking at the first appearance of the word PRESIDENT*, because when someone speaks its name appears in capital letters and in the case of the chamber president the words PRESIDENTE or PRESIDENTA are used. This appearance usually indicates the start of the interventions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findStart(ls:list)->int:\n",
        "    index=0\n",
        "    for line in ls:\n",
        "        if \"PRESIDENT\" in line:\n",
        "            return index\n",
        "        index+=1\n",
        "    return -1\n",
        "\n",
        "def getPublicationText(url:str,term:str,date:str)->str:\n",
        "    import urllib3\n",
        "    # variables\n",
        "    http = urllib3.PoolManager()\n",
        "    # Get the publication\n",
        "    response = http.request('GET', url)\n",
        "    # Parse the publication\n",
        "    parser = PublicationParser()\n",
        "    parser.feed(response.data.decode('utf-8'))\n",
        "\n",
        "    index=findStart(parser.lsData)\n",
        "    text=''\n",
        "    if(index>=0):\n",
        "        # lsData is a list of strings, so we join them all with a space\n",
        "        text=' '.join(parser.lsData[index:])        \n",
        "    else:\n",
        "        os.makedirs(f'data/errors/{term}', exist_ok=True)\n",
        "        with open(f'data/errors/{term}/{date}.html', mode='w') as f:\n",
        "            f.write(response.data.decode('utf-8'))\n",
        "        print(\"Error: PRESIDENT* not found\")\n",
        "    return text\n",
        "\n",
        "def get_speeches(text:str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts the speeches from the text of a publication.\n",
        "    \"\"\"\n",
        "\n",
        "    # This regex finds the name of the politician that is speaking.\n",
        "    # The name is usually in the form of:\n",
        "    # [text]... ALL CAPS SURNAME (the title if president or candidate):\n",
        "    # So we use this simple regex to find the next ALL CAPS that may be \n",
        "    # followed by a parenthesis and ends with a colon.\n",
        "    regexfinder = r'(?:(?:[A-ZÀ-Ü,])(?:-|\\s)?)+(?:\\s*\\([^\\)]*\\))? *:'\n",
        "    # TODO: there's still some special cases like 'DEL DIPUTADO DON ALBERTO GARZÓN ESPINOSA, DEL GRUPO PARLAMENTARIO DE IU, ICV-EUiA, CHA:' that are detected as 'A, CHA'\n",
        "    # Look file data/pagecache/X/20150121.txt\n",
        "\n",
        "    indexes=[(m.start(0),m.end(0)) for m in re.finditer(regexfinder,text, re.U|re.M)]\n",
        "    # print(indexes)\n",
        "\n",
        "    sentences=pd.DataFrame(columns=['Name','Text'])\n",
        "    last=len(indexes)-1\n",
        "    for i in range(len(indexes)):\n",
        "        name=text[indexes[i][0]:indexes[i][1]-1]   \n",
        "\n",
        "        firstIdx=indexes[i][1]+1\n",
        "        if(i<last):\n",
        "            # try to find the end of the last sentence\n",
        "            lastIdx=indexes[i+1][0]\n",
        "            while(text[lastIdx]!='.' and text[lastIdx]!=')'):\n",
        "                lastIdx=lastIdx-1\n",
        "                if(lastIdx<=firstIdx):\n",
        "                    # end of the last sentence not found, take it full.\n",
        "                    lastIdx=indexes[i+1][0]\n",
        "                    break\n",
        "        else:\n",
        "            lastIdx=len(text)\n",
        "        # print(f\"{firstIdx}-{lastIdx}\")\n",
        "        sentence=text[firstIdx:lastIdx].strip()\n",
        "        sentences.loc[len(sentences)]=[name,sentence]\n",
        "    return sentences\n"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1670796357399
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `parse_speeches` function will download all the pages from a term and will parse them using the functions above to extract the\r\n",
        "texts for the full debate. It still does not do a full preparation, but once this process has finished we have a full dataset for each term with the texts of every day that a debate occured."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_speeches(term:pd.DataFrame)->pd.DataFrame:\n",
        "    from alive_progress import alive_bar\n",
        "    import os\n",
        "\n",
        "    speeches_ds = None\n",
        "\n",
        "    with alive_bar(len(term),title=f'importing Term {term.loc[0][\"term\"]}',force_tty=True) as bar:\n",
        "        bar.text='open'\n",
        "        for index,r in term.iterrows():\n",
        "            if os.path.isfile(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt'):\n",
        "                bar.text=f'c-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "                with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt','r') as f:\n",
        "                    text=f.read()\n",
        "            else:\n",
        "                bar.text=f'u-{r[\"term\"]}-{r[\"fecha\"]}'\n",
        "                text=getPublicationText(r[\"url\"],f'{r[\"term\"]}',f'{r[\"fecha\"]}')\n",
        "                os.makedirs(f'data/pagecache/{r[\"term\"]}',exist_ok=True)\n",
        "                with open(f'data/pagecache/{r[\"term\"]}/{r[\"fecha\"]}.txt',mode='w') as f:\n",
        "                    f.write(text)\n",
        "\n",
        "            bar.text=f'Parse {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            speeches=get_speeches(text)\n",
        "            bar.text='Append data {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            speeches[\"Date\"]=r[\"fecha\"]\n",
        "            speeches[\"Term\"]=r[\"term\"]\n",
        "            bar.text=f'Merge {r[\"term\"]}-{r[\"fecha\"]}'\n",
        "            if speeches_ds is None:\n",
        "                speeches_ds=speeches\n",
        "            else:\n",
        "                speeches_ds=pd.concat([speeches_ds,speeches])\n",
        "            bar()\n",
        "    return speeches_ds\n"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1670796357575
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "os.makedirs('data/debates', exist_ok=True)\n",
        "\n",
        "for term_file in glob.glob('data/terms/*.csv'):\n",
        "    base_name = os.path.basename(term_file)\n",
        "    current_term=pd.read_csv(term_file)\n",
        "    term_id=current_term.loc[0]['term']\n",
        "    ds=parse_speeches(current_term)\n",
        "    ds.to_csv(f'data/debates/speeches_term_{term_id}.csv',index=False)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "importing Term X |████████████████████████████████████████| 315/315 [100%] in 2:12.1 (2.39/s)                           \nimporting Term XI |████████████████████████████████████████| 15/15 [100%] in 3.6s (4.20/s)                              \nimporting Term XII |████████████████████████████████████████| 185/185 [100%] in 1:17.1 (2.40/s)                         \nimporting Term XIII |████████████████████████████████████████| 15/15 [100%] in 4.8s (3.12/s)                            \nimporting Term XIV |████████████████████████████████████████| 221/221 [100%] in 1:40.7 (2.19/s)                         \nimporting Term V |████████████████████████████████████████| 197/197 [100%] in 1:21.9 (2.40/s)                           \nimporting Term VI |████████████████████████████████████████| 286/286 [100%] in 2:05.5 (2.28/s)                          \nimporting Term VII |████████████████████████████████████████| 310/310 [100%] in 2:07.4 (2.43/s)                         \nimporting Term VIII |████████████████████████████████████████| 315/315 [100%] in 1:59.4 (2.64/s)                        \nimporting Term IX |████████████████████████████████████████| 282/282 [100%] in 1:46.0 (2.66/s)                          \n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1670797251750
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "77df4a96c0be8bd891423502f384941e461f81b0e784fdbc8bb136153927d3d2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}